TODO: 


03/09/2018
-----------------------
TensorInfo types:
  * how will the tensors get and set data? templates?

Rethink the step(int) function. 
Think about backend abstraction.


06/09/2018
-----------------------
consolidate inferInfo, write inferInfo for AveragePool, re-organise ConvOp
so that as much as possible is done in the constructor

AveragePoolOp setup is basically the same as for ConvOp, even dilation should
be kept (pytorch has it). Only ngroups which should go.


09/09/2018
-----------------------
Loss, streaming off, separate program run every n iterations. If 
a requested tensor has been optimised out, just say too bad to the 
user and move on

10/09/2018
-----------------------
Loss should be speicific to NegLogLikelihood, means can use p (p-1) for gradients 
directly

Loss should take in values before probabilities, and output loss and gradients of
these. General rule for loss nodes : return gradient of input

16/09/2018
-----------------------
expunge dynamic_cast.

19/09/2018
-----------------------
rewrite code to create backwards pass, avoiding lambda functions (make them
class functions) and spawning preferably 1 node per gradient to compute, with
the option for nodes with several output gradient tensors. 

22/09/2018
-----------------------
found a bug in removing a node. I should have a function (like in poponnx)
which does a thorough test of the state of graph. This function can 
be used for debugging purposed to find when the inconsistency was introduced.

23/09
-----------------------
(done) I don't like the implementation of loss. It should be possible to have 
several losses. Change so that this is the case AND make this the default in 
basic

25/09
-----------------------
(done) spent much of the day fighting pytorch. No log op, no gather op, makes the
way I want to implement neuralnet impossible. I will have to go with a vector
of losses, which takes tensors and output a single scalar. These will be summed
to get the total loss. 

27/09
----------------------
(done) optimise out sum with 1 arg
(done) full circle, we're back to the weight update node

28/09
----------------------
(partially done) make optimisations iron-clad. Class to inherit from? 
Decide how the user states which tensors they want back (are anchored)
(done) Visualisation tool, see the liveness through time. 
(done) AddGrad can be optimised out
(done) Pruning at end of graph construction to keep only the targets
ops (of Graph) should be encapsulated, as tensors
(done) recomputation.......

01/10
----------------------
(done) checkpoint determination
inplace relu and inplace add
a local max liveness optimiser.
graph image beautification.
option to disable training target
naming pytorch parameters
training plan : 
*----------->*
|            |
|            |
|            |
|            |
v            v
*----------->*
where left->right   : exporting from pytorch
and   top ->bottom  : training 


05/10
------------------------
consumer topoCons is a DAG! and there is an
elegant way to merge DAGs in the optimization pass

08/10
(done) recomputation: initial partition using sqrt rule should use memory output

19/10
static compilation: all nodes not on path from stream input MUST be static
compiled. 
